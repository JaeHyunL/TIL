{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('data/train.csv', encoding='utf8')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] 2323\n",
      "    id keyword  location                                               text  \\\n",
      "0    1     NaN      3268  Our Deeds are the Reason of this #earthquake M...   \n",
      "1    4     NaN      3268             Forest fire near La Ronge Sask. Canada   \n",
      "2    5     NaN      3268  All residents asked to 'shelter in place' are ...   \n",
      "3    6     NaN      3268  13,000 people receive #wildfires evacuation or...   \n",
      "4    7     NaN      3268  Just got sent this photo from Ruby #Alaska as ...   \n",
      "5    8     NaN      3268  #RockyFire Update => California Hwy. 20 closed...   \n",
      "6   10     NaN      3268  #flood #disaster Heavy rain causes flash flood...   \n",
      "7   13     NaN      3268  I'm on top of the hill and I can see a fire in...   \n",
      "8   14     NaN      3268  There's an emergency evacuation happening now ...   \n",
      "9   15     NaN      3268  I'm afraid that the tornado is coming to our a...   \n",
      "10  16     NaN      3268        Three people died from the heat wave so far   \n",
      "11  17     NaN      3268  Haha South Tampa is getting flooded hah- WAIT ...   \n",
      "12  18     NaN      3268  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
      "13  19     NaN      3268            #Flood in Bago Myanmar #We arrived Bago   \n",
      "14  20     NaN      3268  Damage to school bus on 80 in multi car crash ...   \n",
      "15  23     NaN      3268                                     What's up man?   \n",
      "16  24     NaN      3268                                      I love fruits   \n",
      "17  25     NaN      3268                                   Summer is lovely   \n",
      "18  26     NaN      3268                                  My car is so fast   \n",
      "19  28     NaN      3268                       What a goooooooaaaaaal!!!!!!   \n",
      "20  31     NaN      3268                             this is ridiculous....   \n",
      "21  32     NaN      3268                                  London is cool ;)   \n",
      "22  33     NaN      3268                                        Love skiing   \n",
      "23  34     NaN      3268                              What a wonderful day!   \n",
      "24  36     NaN      3268                                           LOOOOOOL   \n",
      "25  37     NaN      3268                     No way...I can't eat that shit   \n",
      "26  38     NaN      3268                              Was in NYC last week!   \n",
      "27  39     NaN      3268                                 Love my girlfriend   \n",
      "28  40     NaN      3268                                          Cooool :)   \n",
      "29  41     NaN      3268                                 Do you like pasta?   \n",
      "30  44     NaN      3268                                           The end!   \n",
      "31  48  ablaze       453  @bbcmtd Wholesale Markets ablaze http://t.co/l...   \n",
      "32  49  ablaze       922  We always try to bring the heavy. #metal #RT h...   \n",
      "33  50  ablaze       209  #AFRICANBAZE: Breaking news:Nigeria flag set a...   \n",
      "34  52  ablaze      2054                 Crying out for more! Set me ablaze   \n",
      "35  53  ablaze      1516  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...   \n",
      "36  54  ablaze      2114  @PhDSquares #mufc they've built so much hype a...   \n",
      "37  55  ablaze      2864  INEC Office in Abia Set Ablaze - http://t.co/3...   \n",
      "38  56  ablaze      3268  Barbados #Bridgetown JAMAICA ¬â√õ√í Two cars set ...   \n",
      "39  57  ablaze      2020                             Ablaze for you Lord :D   \n",
      "40  59  ablaze      1493  Check these out: http://t.co/rOI2NSmEJJ http:/...   \n",
      "41  61  ablaze      3268  on the outside you're ablaze and alive\\nbut yo...   \n",
      "42  62  ablaze      3117  Had an awesome time visiting the CFC head offi...   \n",
      "43  63  ablaze      3268       SOOOO PUMPED FOR ABLAZE ???? @southridgelife   \n",
      "44  64  ablaze      3268  I wanted to set Chicago ablaze with my preachi...   \n",
      "45  65  ablaze      3268  I gained 3 followers in the last week. You? Kn...   \n",
      "46  66  ablaze      1012  How the West was burned: Thousands of wildfire...   \n",
      "47  67  ablaze      3268  Building the perfect tracklist to life leave t...   \n",
      "48  68  ablaze      1493  Check these out: http://t.co/rOI2NSmEJJ http:/...   \n",
      "49  71  ablaze       905  First night with retainers in. It's quite weir...   \n",
      "\n",
      "    target  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "5        1  \n",
      "6        1  \n",
      "7        1  \n",
      "8        1  \n",
      "9        1  \n",
      "10       1  \n",
      "11       1  \n",
      "12       1  \n",
      "13       1  \n",
      "14       1  \n",
      "15       0  \n",
      "16       0  \n",
      "17       0  \n",
      "18       0  \n",
      "19       0  \n",
      "20       0  \n",
      "21       0  \n",
      "22       0  \n",
      "23       0  \n",
      "24       0  \n",
      "25       0  \n",
      "26       0  \n",
      "27       0  \n",
      "28       0  \n",
      "29       0  \n",
      "30       0  \n",
      "31       1  \n",
      "32       0  \n",
      "33       1  \n",
      "34       0  \n",
      "35       0  \n",
      "36       0  \n",
      "37       1  \n",
      "38       1  \n",
      "39       0  \n",
      "40       0  \n",
      "41       0  \n",
      "42       0  \n",
      "43       0  \n",
      "44       0  \n",
      "45       0  \n",
      "46       1  \n",
      "47       0  \n",
      "48       0  \n",
      "49       0  \n"
     ]
    }
   ],
   "source": [
    "# ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "location_ohe = ohe.fit_transform(train[['keyword']].fillna(\"unknown\"))\n",
    "# train['keyword'] = pd.to_numeric(train['keyword'], errors='coerce')\n",
    "print(location_ohe, '2323')\n",
    "train['location'] = le.fit_transform(train['location'].fillna(\"unknown\"))\n",
    "# train['location'] = pd.to_numeric(train['location'], errors='coerce')\n",
    "print(train.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7740345110928513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "X = train['text']     # ÌÖçÏä§Ìä∏ Ïª¨Îüº\n",
    "y = train['target']   # Ï†ïÎãµ (0 ÎòêÎäî 1)\n",
    "# Word2Vec Î™®Îç∏ Îã§Ïãú Î°úÎî© Î∞è ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî Ïû¨Ïã§Ìñâ\n",
    "\n",
    "# Word2Vec ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ Î°úÎî©\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "\n",
    "# ÌèâÍ∑† Î≤°ÌÑ∞ ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "def get_average_word2vec(text, model, vector_size=300):\n",
    "    words = text.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Ìôî\n",
    "X_vec = np.vstack([get_average_word2vec(text, word2vec_model) for text in X])\n",
    "\n",
    "\n",
    "# X_feats = train['location'].values\n",
    "# print(train.head(50), '@#@#@#123123')\n",
    "# X_combined = np.hstack([X_vec, X_feats, location_ohe])\n",
    "X_combined = np.hstack([X_vec, location_ohe])\n",
    "\n",
    "# ÌïôÏäµ/Í≤ÄÏ¶ù Î∂ÑÌï† Î∞è VotingClassifier Ïû¨Ïã§Ìñâ\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# Ïã§Ìñâ Í∞ÄÎä•Ìïú Î™®Îç∏Î°ú Voting Íµ¨ÏÑ±\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "svc_model = SVC(probability=True)\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "estimators = [\n",
    "    ('lr', lr_model),\n",
    "    ('svc', svc_model),\n",
    "    ('rf', rf_model)\n",
    "]\n",
    "voting_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred = voting_model.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "# ====== ROUND1 ===========\n",
    "# Stacking ÏïôÏÉÅÎ∏î Í∏∞Î≤ïÏÇ¨Ïö©Ïãú.\n",
    "# üìä F1 Score: 0.7044\n",
    "# ====== ROUND2 ===========\n",
    "# VotingClassifier ÏïôÏÉÅÎ∏î Í∏∞Î≤ïÏÇ¨Ïö©Ïãú.\n",
    "# üìä F1 Score: 0.6951\n",
    "# ====== ROUND3 ===========\n",
    "# VotingClassifier + word2vec_model Encoding\n",
    "# üìä F1 Score: 0.7556675062972292\n",
    "\n",
    "# ====== ROUND4 ===========\n",
    "# VotingClassifier + word2vec_model Encoding + OneHotEncoding Labeling\n",
    "# üìä F1 Score: 0.7740345110928513\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "X_test = test_df['text'].fillna(\"\")\n",
    "\n",
    "# Word2Vec ÌèâÍ∑† Î≤°ÌÑ∞ Ìï®Ïàò (Ïù¥ÎØ∏ Î°úÎìúÎêú Î™®Îç∏ Í∏∞Ï§Ä)\n",
    "def get_average_word2vec(text, model, vector_size=300):\n",
    "    words = text.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "X_test_vec = np.vstack([get_average_word2vec(text, word2vec_model) for text in X_test])\n",
    "\n",
    "location_ohe = ohe.fit_transform(test[['keyword']].fillna(\"unknown\"))\n",
    "X_combined = np.hstack([X_test_vec, location_ohe])\n",
    "\n",
    "# ÏòàÏ∏°\n",
    "test_preds = voting_model.predict(X_combined)\n",
    "\n",
    "# Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ±\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
