{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('data/train.csv', encoding='utf8')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] 2323\n",
      "    id keyword  location                                               text  \\\n",
      "0    1     NaN      3268  Our Deeds are the Reason of this #earthquake M...   \n",
      "1    4     NaN      3268             Forest fire near La Ronge Sask. Canada   \n",
      "2    5     NaN      3268  All residents asked to 'shelter in place' are ...   \n",
      "3    6     NaN      3268  13,000 people receive #wildfires evacuation or...   \n",
      "4    7     NaN      3268  Just got sent this photo from Ruby #Alaska as ...   \n",
      "5    8     NaN      3268  #RockyFire Update => California Hwy. 20 closed...   \n",
      "6   10     NaN      3268  #flood #disaster Heavy rain causes flash flood...   \n",
      "7   13     NaN      3268  I'm on top of the hill and I can see a fire in...   \n",
      "8   14     NaN      3268  There's an emergency evacuation happening now ...   \n",
      "9   15     NaN      3268  I'm afraid that the tornado is coming to our a...   \n",
      "10  16     NaN      3268        Three people died from the heat wave so far   \n",
      "11  17     NaN      3268  Haha South Tampa is getting flooded hah- WAIT ...   \n",
      "12  18     NaN      3268  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
      "13  19     NaN      3268            #Flood in Bago Myanmar #We arrived Bago   \n",
      "14  20     NaN      3268  Damage to school bus on 80 in multi car crash ...   \n",
      "15  23     NaN      3268                                     What's up man?   \n",
      "16  24     NaN      3268                                      I love fruits   \n",
      "17  25     NaN      3268                                   Summer is lovely   \n",
      "18  26     NaN      3268                                  My car is so fast   \n",
      "19  28     NaN      3268                       What a goooooooaaaaaal!!!!!!   \n",
      "20  31     NaN      3268                             this is ridiculous....   \n",
      "21  32     NaN      3268                                  London is cool ;)   \n",
      "22  33     NaN      3268                                        Love skiing   \n",
      "23  34     NaN      3268                              What a wonderful day!   \n",
      "24  36     NaN      3268                                           LOOOOOOL   \n",
      "25  37     NaN      3268                     No way...I can't eat that shit   \n",
      "26  38     NaN      3268                              Was in NYC last week!   \n",
      "27  39     NaN      3268                                 Love my girlfriend   \n",
      "28  40     NaN      3268                                          Cooool :)   \n",
      "29  41     NaN      3268                                 Do you like pasta?   \n",
      "30  44     NaN      3268                                           The end!   \n",
      "31  48  ablaze       453  @bbcmtd Wholesale Markets ablaze http://t.co/l...   \n",
      "32  49  ablaze       922  We always try to bring the heavy. #metal #RT h...   \n",
      "33  50  ablaze       209  #AFRICANBAZE: Breaking news:Nigeria flag set a...   \n",
      "34  52  ablaze      2054                 Crying out for more! Set me ablaze   \n",
      "35  53  ablaze      1516  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...   \n",
      "36  54  ablaze      2114  @PhDSquares #mufc they've built so much hype a...   \n",
      "37  55  ablaze      2864  INEC Office in Abia Set Ablaze - http://t.co/3...   \n",
      "38  56  ablaze      3268  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...   \n",
      "39  57  ablaze      2020                             Ablaze for you Lord :D   \n",
      "40  59  ablaze      1493  Check these out: http://t.co/rOI2NSmEJJ http:/...   \n",
      "41  61  ablaze      3268  on the outside you're ablaze and alive\\nbut yo...   \n",
      "42  62  ablaze      3117  Had an awesome time visiting the CFC head offi...   \n",
      "43  63  ablaze      3268       SOOOO PUMPED FOR ABLAZE ???? @southridgelife   \n",
      "44  64  ablaze      3268  I wanted to set Chicago ablaze with my preachi...   \n",
      "45  65  ablaze      3268  I gained 3 followers in the last week. You? Kn...   \n",
      "46  66  ablaze      1012  How the West was burned: Thousands of wildfire...   \n",
      "47  67  ablaze      3268  Building the perfect tracklist to life leave t...   \n",
      "48  68  ablaze      1493  Check these out: http://t.co/rOI2NSmEJJ http:/...   \n",
      "49  71  ablaze       905  First night with retainers in. It's quite weir...   \n",
      "\n",
      "    target  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "5        1  \n",
      "6        1  \n",
      "7        1  \n",
      "8        1  \n",
      "9        1  \n",
      "10       1  \n",
      "11       1  \n",
      "12       1  \n",
      "13       1  \n",
      "14       1  \n",
      "15       0  \n",
      "16       0  \n",
      "17       0  \n",
      "18       0  \n",
      "19       0  \n",
      "20       0  \n",
      "21       0  \n",
      "22       0  \n",
      "23       0  \n",
      "24       0  \n",
      "25       0  \n",
      "26       0  \n",
      "27       0  \n",
      "28       0  \n",
      "29       0  \n",
      "30       0  \n",
      "31       1  \n",
      "32       0  \n",
      "33       1  \n",
      "34       0  \n",
      "35       0  \n",
      "36       0  \n",
      "37       1  \n",
      "38       1  \n",
      "39       0  \n",
      "40       0  \n",
      "41       0  \n",
      "42       0  \n",
      "43       0  \n",
      "44       0  \n",
      "45       0  \n",
      "46       1  \n",
      "47       0  \n",
      "48       0  \n",
      "49       0  \n"
     ]
    }
   ],
   "source": [
    "# 피처 엔지니어링\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "location_ohe = ohe.fit_transform(train[['keyword']].fillna(\"unknown\"))\n",
    "# train['keyword'] = pd.to_numeric(train['keyword'], errors='coerce')\n",
    "print(location_ohe, '2323')\n",
    "train['location'] = le.fit_transform(train['location'].fillna(\"unknown\"))\n",
    "# train['location'] = pd.to_numeric(train['location'], errors='coerce')\n",
    "print(train.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7740345110928513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "X = train['text']     # 텍스트 컬럼\n",
    "y = train['target']   # 정답 (0 또는 1)\n",
    "# Word2Vec 모델 다시 로딩 및 텍스트 벡터화 재실행\n",
    "\n",
    "# Word2Vec 사전학습 모델 로딩\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "\n",
    "# 평균 벡터 임베딩 함수\n",
    "def get_average_word2vec(text, model, vector_size=300):\n",
    "    words = text.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "# 텍스트 벡터화\n",
    "X_vec = np.vstack([get_average_word2vec(text, word2vec_model) for text in X])\n",
    "\n",
    "\n",
    "# X_feats = train['location'].values\n",
    "# print(train.head(50), '@#@#@#123123')\n",
    "# X_combined = np.hstack([X_vec, X_feats, location_ohe])\n",
    "X_combined = np.hstack([X_vec, location_ohe])\n",
    "\n",
    "# 학습/검증 분할 및 VotingClassifier 재실행\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# 실행 가능한 모델로 Voting 구성\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "svc_model = SVC(probability=True)\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "estimators = [\n",
    "    ('lr', lr_model),\n",
    "    ('svc', svc_model),\n",
    "    ('rf', rf_model)\n",
    "]\n",
    "voting_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred = voting_model.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "# ====== ROUND1 ===========\n",
    "# Stacking 앙상블 기법사용시.\n",
    "# 📊 F1 Score: 0.7044\n",
    "# ====== ROUND2 ===========\n",
    "# VotingClassifier 앙상블 기법사용시.\n",
    "# 📊 F1 Score: 0.6951\n",
    "# ====== ROUND3 ===========\n",
    "# VotingClassifier + word2vec_model Encoding\n",
    "# 📊 F1 Score: 0.7556675062972292\n",
    "\n",
    "# ====== ROUND4 ===========\n",
    "# VotingClassifier + word2vec_model Encoding + OneHotEncoding Labeling\n",
    "# 📊 F1 Score: 0.7740345110928513\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 테스트 데이터 불러오기\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "X_test = test_df['text'].fillna(\"\")\n",
    "\n",
    "# Word2Vec 평균 벡터 함수 (이미 로드된 모델 기준)\n",
    "def get_average_word2vec(text, model, vector_size=300):\n",
    "    words = text.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "# 테스트 데이터를 벡터로 변환\n",
    "X_test_vec = np.vstack([get_average_word2vec(text, word2vec_model) for text in X_test])\n",
    "\n",
    "location_ohe = ohe.fit_transform(test[['keyword']].fillna(\"unknown\"))\n",
    "X_combined = np.hstack([X_test_vec, location_ohe])\n",
    "\n",
    "# 예측\n",
    "test_preds = voting_model.predict(X_combined)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
