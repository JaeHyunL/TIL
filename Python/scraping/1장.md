# 1장

## 1.1 연결

웹 사이트는 하나의 페이지가 여러개의 파일로 구성 돼있음

```python
from urllib.request import urlopen
html = urlopen("<http://pythonscraping.com/pages/page1.html>")
print(html.read())
```

## 1.2 BeautifulSoup 소개

bs4라는 외부 라이브러리를 사용함 (HTML 이나 XML 을 쉽게 탐색 할 수 있음 구문분석기)

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen("<http://pythonscraping.com/pages/page1.html>")
bs = BeautifulSoup(html.read(), 'html.parser')
print(bs.h1)
```

html 을 읽어온다음 bs4 모듈을 통해 h1 태그를 가져오는 로직

BeatifulSoup에 첫 번째 매개변수는 html 텍스트이고, 두 번째 매개변수는 구문 분석기임

**‘lxml;’은 형식을 정확하게 지키지 않은 지저분한 html 코드를 분석할때 htmlparesr 보다 조금 더 나음 lxml은 닫히지 않은 태그 계층 구조가 잘못돠된 태크, 헤더나 바디태그가 없는 등의 문제에서 일일히 멈추지않고 그 문제를 수정함 lxml은 html.parser에 비해 조금 더 빠르기는 하지만**

네트워크 병목현상 문제로 별 다른 의미는 없음.

lxml을 사용하기 위해서인 서드파티 C 언어 라이브러리가 있어야 제대로 동작

html5lib도 널리 쓰이는 HTML 구문 분석기 임 lxml 보다 조금 더 나은 에러 수정을 수행 하긴 함

### 1.2.3 신뢰할 수 있는 연결과 예외 처리

데이터 형식은 제대로 지켜지지 않고 웹사이트는 자주 다운되며 닫는 태그도 종종 빠져 잇음,

스크레이퍼의 임포트 문 다음 행을 살펴보고 예외를 어떻게 처리 할 지 생각 해보자.

```python
html = urlopen("<http://www.pythonscraping.com/pages/page1.html>")
```

해당 코드에서 예외가 생길 경우는

- 페이지를 찾을 수 없거나 , URL 해석에서 에러가 생긴 경우
- 서버를 찾을 수 없는 경우

첫 번쨰 상황에서는  http 에러가 반환 될 것

```python
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup

def getTitle(url):
    try:
        html = urlopen(url)
    except HTTPError as e:
        return None
    try:
        bs = BeautifulSoup(html.read(), 'html.parser')
        title = bs.body.h1
    except AttributeError as e:
        return None
    return title

title = getTitle("<http://www.pythonscraping.com/pages/page1.html>")
if title == None:
    print("Title could not be found")
else:
    print(title)
```

페이지 타이틀을 반환하거나 어떤 문제가 있으면 None 객체를 반화는 getTitle 함수를 만듬 getTitle 내부에서는 이전 에제와 마찬가지로 HTTPError를 체크하고 BeautifulSoup 행두개를 try 문으로 캡슐화 함