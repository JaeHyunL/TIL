# 용어

**인덱스**: 데이터 저장공간 하나의 타입만 가지며 물리적인 노드에 여러개의 논리적인 인덱스를 생성할 수 있다.

**샤드**: 색인된 데이터는 물리적인 공간에 여러개 파티션으로 나뉘어 구성되는데 이 파티션을 샤드라고 부른다.

**타입**: 논리적 구조를 의미하며 인덱스 속성에 따라 분류하기도 함. 1인덱스 1타입.

**문서**: 데이터가 저장되는 최소 단위. JSON 데이터가 저장됨.

**필드**: 필드는 문서를 구성하기 위한 속성 데이터베이스의 칼럼과 유사할 수 있으나 보다 동적인 데이터 타입이라고 할 수 있음.

**매핑**: 문서의 필드와 필드의 속성을 정의하고 그에 따른 색인 방법을 정의하는 프로세스이다.

**노드**: 엘라스틱 서치의 경우 다수의 서버로 분산해서 처리하는것이 가능하기에

- 마스터 노드
  - 클러스터를 관리한다.
  - 노드 추가와 제거 같은 클러스터의 전반적인 관리 담당.
- 데이터 노드
  - 실질적인 데이터를 저장한다
  - 검색과 통계 같은 데이터 관련 작업을 수행행한다.
- 코디네이팅 노드
  - 사용자의 요청만 받아서 처리한다.
  - 검색과 통계같은 데이터 관련 작업을 수행한다.
- 인제스트 노드
  - 문서의 전처리 작업을 담당한다.
  - 인덱스 생성 전 문서의 형식을 다양하게 변경할 수 있다.

**마스터 노드**: 인덱스 생성 삭제 관련 작업을 담당하며 네트워크 속도가빠르고 지연없이 없는 노드를 마스터 노드로 생성해야 한다. 마스터 노드는 오직 한개.

**데이터 노드**: 데이터가 실제로 분산 저장되는 물리적인 공간인 샤드가 배치되는 노드이기도 하다. 컴퓨팅 리소스를 많이 소모하기 때문에 리소스 모니터링이 필요하다. 가능한 마스터 노드와 분리해서 구성.

**코디네이팅 노드**: 들어온 요청을 단순히 라운드로빈 방식으로 분산 시켜주는 노드이다.

**인제스트 노드**: 색인 앞에서 데이터를 전처리 하기 위한 노드이다. 스크립트로 전처리 파이프 라인을 구성할 수 있다.

**데이터 집계타임** : 4가지 API로 제공 됨.

- 버킷 집계
  - 집계 중 가장많이 사용한다. 문서의 필드를 기준으로 버킷을 집계한다.
- 메트릭 집계
  - 문서에서 추출된 값을 가지고 Sum, Max, Min, avg를 계산한다.
- 매트릭스 집계
  - 행렬의 값을 합하거나 곱한다.
- 파이프라인 집계
  - 버킷에서 도출된 결과 문서를 다른 필드 값으로 재 분류한다. 생성된 출력 결과를 다시 한번 집계한다. (패싯과 의 차이)

> 형태소 분석이 가능한 필드는 Text 아니면 Keyword나 다른 컬럼을 사용해도 됨 좀 더 읽어봐야할 듯.

**analyzer** : 해당필드의 데이터를 분석하겠다는 의미에 파라미터다. 색인과 검색 시 지정한 분석기로 형태소 분석을 수행한다. text 데이터 타입의 필드는 analyerzer 매핑 파라미터를 기본적으로 사용해야 한다.

**normalizer**: 매핑 파라미터는 term query에 분석기를 사용하기 위해 사용된다. 예를 들어 keyword 데이터 타입의 경우 원문을 기준으로문서가 색인되기 때문에 cafe, Cafe, CaFe는 서로 다른 문서로 인식된다. 하지만 해당 유형을 normalizer를 통해 분석기에 asciifolding과 같은 필터를 사용하면 같은 데이터로 인식되게 할 수 있다.

**boost**: 필드에 가중치를 부여한다. 가중치에 따라 유사도 점수가 달라지기 때문에 boost 설정 시 검색 결과의 노출 순서에 영향을 준다. 만약 색인 시점에 boost 설정을 하게 된다면 재색인하지 않는 이상 가중치를 변경할 수 없기 떄문에사용해야한다. 가급적 검색 시점에만 사용하는 것을 권장.

**corece**: 색인 시 자동변환을 허용할지 여부를 설정하는 파라미터다. 예를 들어 “10”과 같은 숫자 형태의 문자열이 integer 타입의 필드에 들어온다면 엘라스틱서치는 자동으로 형변환을 수행해서 정장적으로 처리한다. 하지만 corece 설정을 미사용으로 변경한다면 색인에 실패할 것이다.

**copy_to**: 매핑 파라미터를 추가한 필드의 값을 지정한 필드로 복사한다. 예컨대 keyword 타입의 필드에 copy_to 매핑 파라미터를 사용해 다른 필드로 값을 복사하면 복사된 필드에서는 text 타입을 지정해 형태소 분석을 할 수도 있따.

**fielddata**: 엘라스틱서치가 힙 공간에 생성하는 매모리 캐시다. doc_values라는 새로운 형태의 캐스를 제공하고 있으며, text 타입의 필드를 제외한 모든 필드는 기본적으로 doc_values 캐시를 사용한다. fielddata를 사용해야만 하는 경우도 있다. text 타입의 필드는 기본적으로 분석기에 형태소 분석이 되기 때문에 집계나 정렬 등의 기능을 수행할 수 없다. 하지만 부득이하게 text 타입의 필드에서나 집계나 정렬을 수행하는 경우도 있을 것이다. 이러한 경우에 한해 fielddata를 사용할 수 있다. 하지만 fielddata는 메모리에 생성되는 캐시이기 때문에 최소한으로만 사용해야 한다는 사실에 주의 해야 한다.fielddata는 메모리 소모가 크기 때문에 기본적으로 비활성화 돼ㅔ있다.

**dynamic** : 매핑에 필드를 추가 할때 동적으로 생성할지 생성하지 않을지를 결정한다. 

**position_increment_gap**: 배열 형태의 데이터를 색인할 때 검색의 정확도를 높이기 위해 제공하는 옵션이다. 필드 데이터 중 단어와 단어 사이의 간격(slop)을 허용할지를 설정한다. 검색 시 단어와 단어 사이의 간격을 기준으로 일치하는 문서를 찾는데 필요하다

**properties**: 오브젝트 타입이나 중첩 타입의 스키마를 정의할 떄 사용되는 옵션으로 필드의 타입을 매핑한다. 오브젝트 필드 및 중첩 필드에는 properties라는 서브 필드가 있따. 이 properties는 object나 nested를 포함한 모든 데이터 타입이 될 수 있다.

**search_analyzer**: 일반적으로 색인과 검색 시 같은 분석기를 사용한다 만약 다른 분석기를 사용하고 싶은 경우 search_analyzer를 설정해서 검색 시 사용할 분석기를 별도로 지정할 수 있다.

**similarity**: 유사도 측정 알고리즘을 지정한다.

**store**: 필드의 값을 저장해 검색 결과에 값을 포함하기 위한 매핑 파라미터이다.

**term_vector**: 루씬에서 분석된 용어의 정보를 포함할지 여부를 결정하는 매핑 파라미터 이다.

### 메타필드

엘라스틱서치에서 생성한 문서에서 제공하는 특별 필드. 메타 데이터를 저장하는 특수 목적의 필드로서 이를 이용하면 검색 시 문서를 다양한 형태로 제어하는 것이 가능해진다.

**_index**: 메타 필드 메타필드는 해당 문서가 속한 인덱스의 이름을 담고 있다. 이를 이용해 검색된 문서의 인덱스 명을 알 수 있으며, 해당 인덱스에 몇개의 문서가 있는지 확인 할 수 있따.

**_type**: 해당문서가 속한 매핑의 타입 정보를 담고 있따. 이를 이용해 해당 인덱스 내부에서 타입별로 몇 개의 문서가 있는지 확인할 수 있다.

**_id**: 문서를 식별하는 유일한 키 값이다. 인덱스에서 색인된 문서마다 서로 다른 키 값을 가진다.

**_uid**: 문서 특수한 목적의 식별키다 “#”해시태그를  사용해 _type과 _id 값을 조합해 사용한다. 내부적으로 사용되기 때문에 검색시 조회되는 값은 아니다.

**_source**: 문서의 원본 데이터를 제공한다. 내부에는 색인 시 전달되는 JSON 문서의 본문이 저장돼 있다. 일반적으로 원본 JSON문서를 검색 결과로 표시할 떄 사용한다.

재색인할 때 스크립트로 prdtYear 필드에 1을 더했다. 재색인되는 인덱스의 필드 데이터 타입이 keyword로 자동생성되므로 문자열 형태로 1이 더해질 것이다.

**_all**: 색인에 사용된 모든 필드의 정보를 가진 메타 필드다. 모든 필드의 내용이 하나의 텍스트로 합쳐져서 제공된다. 필드가아닌 문서 전체 필드에서 특정 키워드를 검색한다면 메타필드를 사용된다.

**_routing**: 특정 문서를 특정 샤드에 저장하기위해 사용자가 저장하는 메타 필드다. 기본적으로 색인하면 해당 문서는 다음 수식에 따라 문서 id를 이용해 문서가 색인될 샤드를 결정한다. 별도의 설정 없이 문서를 색인하면 문서는 샤드에 골고루 분산되어 저장된다.

어떤 경우에는 특정 문서들을 하나의 샤드에 저장하고 싶을 수 있다. 이때 _routing 메타 필드를 사용할 수 있다. 색인할 때 해당 문서들은 동일한 라우팅 ID를 지정한다. 문서 ID를 사용하는 대신 파라미터로 입력한 rotung값이 샤드를 결정하는데 사용된다.

### 3.3 필드 데이터 타입

매핑 설정을 위해서는 엘라스틱서치에서 제공하는 데이터 타입으로 어떠한 종류가 있는지 정확하게 이해하는 것이 중요하다. 이를 바탕으로 데이터의 종류와 형태에 따라 데이터 타입을 선택적으로 사용해야 한다.

Geo_Point, Geo_shape

**keyword 데이터타입**: keyword 데이터 타입은 말 그대로 키워드 형태로 사용할 데이터에 적합한 데이터 타입이다. keyword 타입을 사용할 경우 별도의 분석기를 거치지 않고 원문 그대로 색인하기 때문에 특정 코드나 키워드 등 정영화된 콘테츠에 주로 사용된다. 엘라스틱서치의 일부 기능은 형태소 분석을 하지 않아야만 사용이 가능한데 이 경우에도 keyword 데이터 타입이 사용된다.

**Text**: 색인 시 지정된 분석기가 칼럼의 데이터를 문자열 데이터로 인식하고 이를 분석한다. Text 데이터 타입은 전문 검색이 가능하다는 점이 가장 큰 특징이다. Text 타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 생성되며 검색이 가능해짐.

정렬이나 집계 연산을 사용해야 할 때가 있다. 이러한 경우 Text 타입과 Keyword 타입을 동시에 멀티 필드로 설정할 수 있다.

- **analyzer**: 인덱스와 검색에 사용할 형태소 분석기를 선택한다.
- **boost**: 필드의 가중치로 검색 결과 정렬에 영향을 준다. 기본값은 1.0으로 1보다 크면 점수가 높게 오르고, 적으면 점수가 낮게 오른다.
- **fielddata**: 정렬, 집계, 스크립트 등에서 메모리에 저장된 필드 데이터를 사용할지를 설정한다. 기본값은 false다.
- **index**: 해당 필드를 검색에 사용할지를 설정한다. True다.
- **norms**: 유사도 점수를 산정할 때 필드 길이를 고려할지 결정한다. 기본값은 true다.
- **store**: 필드 값을 필드와 별도로 _source에 저장하고 검색 가능하게 할지를 설정한다. 기본값은 false다.
- **search_analyzer**: 검색에 사용할 형태소 분석기를 선택한다.
- **similarity**: 유사도 점수를 구하는 알고리즘 선택한다. 기본값은 BM25다.
- **term_vector**: Anayzed 필드에 텀벡터를 저장할지를 결정한다. 기본값은 No다.

### 3.3.3 Array 데이터 타입

데이터는 대부분 1차원(하나의 필드에 하나의 값이 매핑)으로 표현되지만 2차원 하나의 필드에 여러개의 값이 매핑으로 존재하는 경우도 있을 것이다. 영화 데이터에 subtitleLang 필드가 있고 해당 필드에는 개봉 영화의 언어 코드 데이터가 들어있다고 가정해 보자. 언어의 값으로 영어(en)와 한국어 (ko)라는 두개의 데이터를 입력하고 싶을 경우 Array 데이터 타입을 사용해야한다.

### 엘라스틱서치 분석기

**Standard Analyzer**: 공백 혹은 특수 기호를 기준으로 토큰을 분리하고 모든 문자를 소문자로 변경하는 토큰 필터를 사용한다.

**WhiteSpace 분석기**: 이 분석기는 공백 문자열을 기준으로 토큰을 분리하는 간단한 분석기다.

**Keyword 분석기**: 입력 문자열을 하나의 키워드처럼 처리한다. 토큰화 작업을 하지 않는다.

**Ngram 토크나이저**: Ngram 토크나이저 Ngram은 기본적으로 한 글자씩 토큰화한다. Ngram에 특정 문자를 지정할 수도 있으며 이 경우 지정된 문자 목록 중 하나를 만들 떄 마다 단어를 자른다. 그밖에도 다양한 옵션을 조합해서 자동완성을 만들 때 유용하게 활용 할 수 있따.

**Edge Nagram 토크나이저**: 자정된 목록 중 하나를 만날 때 마다 시작 부분을 고정시켜 단어를 자르는 방식으로 사용하는 토크나이저.

**Keyword 토크나이저**: 텍스트를 하나의 토큰으로 만듬.

**Token Filter**: 토크나이저에서 분리된 토큰들을 변형 추가 삭제할 떄 사용하는 필터. 토크나이저에 의해 토큰이 모두 분리되면 토큰은 배열 형태로 토큰 필터로 전달된다. 토크 나이저에 의해 모두 분리돼야 비로소 동작

**Ascii Folding 토큰 필터**: 아스키 코드에 해당하는 127개 알파벳 숫자 기호에 해당되지 않는경우 Ascii요소로 변경한다.

 

**Lowercase 토큰 필터**: 이 토큰 필터는 토큰을 구성하는 전체 문자열을 소문자로 변환한다.

**Uppercase 토큰 필터**: ㅌ전체 문자열을 대문자로 변환한다.

**Stop 토큰 필터**: 불용어로 등록할 사전을 구축해서 사용하는 필터를 의미한다. 인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록해서 해당 단어에 대한 불용어 사전을 구축한다.

**Stemmer 토큰 필터**: stemming 알고리즘을 사용해 토큰을 변형하는 필터.

**Synoym 토큰 필터**: 동의어를 처리할 수 있는 필터.

**Triom 토큰 필터**: 앞뒤 공백을 제거하는 토큰 필터.

**동의어 사전 개념**: 엘라스틱서치 에서 가장 까다로운 부분이 동의어를 관리하는 것 검색엔진에서 다루는 분야가 많아지면 동의어 수도 늘어나고 파일도 늘어남. 이러한 동의어를 모아둔 파일이 동의어 사전이라고 함.

**Document API**: 엘리스틱서치에서는 인덱스 관리를 목적으로 Document API를 제공하는데 이를 이용해 문서를 조회하거나 추가, 수정, 삭제 작업을 할 수 있따.

ELK서치에 제공하는 대표적인 Document API

- index API: 문서를 생성
- Get API: 문서를 조회
- Delete API: 문서를 삭제
- Update API: 문서를 수정
- Bulk API: 대량의 문서를 처리
- Reindex API: 문서를 복사

**오퍼레이션 타입**: 일반적으로 ID가 이미 존재하는 경우에는 update 작업이 일어나고, ID가 없을 경우에는 create 작업이 일어난다. 만약 같은 ID로 색인이 반복적으로 이뤄질 경우에는 매번 Update 작업이 일어나게 된다. 

**타임아웃 설정**: 일반적으로 색인을 요청할 때 대부분은 즉시 처리된다. 하지만 이미 색인 작업이 진행 중인 동안 추가적으로 색인 API가 호출 될 경우 즉시 처리 되지 못하고 일정 기간 대기하게 된다. 기본적으로 1분간 대기.

**인덱스 매핑정보 자동 생성**: index API로 문서를 색인할 때 기존에 정의되지 않은 필드의 정보가 존재할 경우 어떻게 해야 할 지 정해야한다. 기본적으로는 동적 매핑을 허용하기 때문에 색인 즉시 새로운 필드가 생성된다.  동적 매핑을 허용할 경우 원하지 않는 오류가 발생할 수도 있기 때문에 상황에 따라 기능을 비활성화 해야한다.

**인덱스 API**: Index API는 문서를 특정 인덱스에 추가하는데 사용된다. _shards 항목은 몇 개의 샤ㅐ드에서 명령이 수행됐는지에 대한 정보를 나타낸다. 여기서 total 항목은 복제돼야 하는 전체 샤드 개수를 나타내며 successful은 성공적으로 복제된 샤드의 갯수를 나타낸다.

**Get API**: getAPI는 특정 문서를 인덱스에서 조회할 때 사용하는 API다. 조회하고자 하는 문서의 ID를 명시적으로 지정해서 사용한다. 일반적으로 조회되는 문서의 내용은 _source 항목으로 확인 할 수 있따>

**DeleteAPI**: Delete API를 이용하면 문서를 삭제할 수 있다. 그결과 result항목에 deleted 라는 값ㅇ ㅣ반환되며, _version 값이 1만큼 증가한 것을 확인할 수 있따.

**Delete By Query API**: 특정 인덱스에서 검색을 수행한 후 그 결과에 해당하는 문서만 삭제하고 싶을 경우 Delete By QUery API를 사용하면 댄다.

QueryAPI를 불러와 호출하면 해당 인덱스의 스냅셔ㅛㅅ을 불러와 문서 버전을 기반으로 삭제를 수행한다.

**Update API**: Update API를 이용하면 스크립트 바탕으로 문서를 수정할 수 있다

**Bulk API**: 한번의 호출로 다수의 문서를 색인하거나 삭제할 수 있따.

**Reindex API**: 한 인덱스에서 다른 인덱스로 문서를 복사할 때다.

---

## Query DSL

**데이터 검색**: 검색을 색인시 말고 조회시 필터를 걸떄 사용하는 방법  QueryDSL을 이용하라. 

**범위 검색**: 범위를 기준으로 질의

| 문법 | 연산자 | 설명 |
| --- | --- | --- |
| lt | < | 피 연산자보다 작음 |
| gt | > | 피 연산자보다 큼 |
| lte | ≤ | 피 연산자보다 작거나 같을 때 |
| gte | ≥ | 피 연산자보다 크거나 같을 때 |

**operator 설정**: 엘라스틱서치는 검색시 문장이 들어올 경우 기본적으로 OR 연산으로 동작한다. 하지만 실무에서는 OR보다 And 연산을 사용해 정확도를 높여 검색해야 할 때가 많을 것이다.

**minimum_should_match 설정**: or 연산을 사용할 경우 검색 결과가 너무 많아 질 수 있다. 이때 몇 개 이상 매칭될 때만 검색 결과로 나오게 할 수 있는데 이때 사용하는 파라미터 이다.

**fuzziness 설정**: fuzziness 파라미터를 사용하면 단순히 같은 값을 찾는 match Query를 유사한 값을 찾는 Fuzzy Query로 변경할 수 있다. 이는 0 기반으로 문서의 필드 값을 여러 번 변경하는 방식으로 동작한다. 한국어보단 영어에 적합 오차범위가 있는 단어 까지 검색해서 보여줌

**boost 설정**

boost 관련 설정은 검색에서 가장 많이 사용하는 파라미터 중 하나이다. 이 파라미터는 관련성이 높은 필드나 키워드에 가중치를 더 줄 수 있게 해준다. 영화 데이터의 경우 한글 영화 제목과 영문 영화 제목으로 두 가지 제목 필드를 제공하고 있다. 이때 한글 영화 제목에 좀 더 가중치를 부여해서 검색 결과를 사요용하고 싶을 때.

**Match Query**: 형태소 분석을 통해 텀으로 분리한 후 이텀을 이용해 검색 질의를 수행한다.

**Multi Match Qurery**: Multi_match 파라미터를 이용하면 Milti Match Query를 할 수 있다. 기본적인 사용 방법은 동일하나 단일플다드가 아닌 여러개읲 ㅣㄹ드를 대상으로 검색해야 할 때 사용하는 쿼리다.

**Term Query**: 텍스트 형태의 값을 검색하기 위해 엘라스틱 서치는 두가지 매핑 유형을 지원한다.

**Bool Query**: 관계형 데이터베이스에서는 AND, OR로 묶은 여러 조건을 Where 절에서 사용할 수 있다.

**Query String**: 엘라스틱서치에는 기본적으로 내장된 쿼리 분석기가 있다. query_string 파라미터를 사용하는 쿼리를 작성하면 내장된 쿼리 분석기를 이용하는 질의를 작성 할 수 있따. 다음 예제를 보자.

**prefix Query**: Prefix Query는 해당 접두어가 있는 모든 문서를 검색하는데 사용한다. 역색인된 텀은 사전순으로 정렬되고 Prefix Query는 저장된 텀들을 스캔해서 일치하는 텀을 찾는다.

**Exists Query**: MovieNm 칼럼에 값이 존재하는 문서만 찾아준다.

**WildCard Query**: 검색어가 와일드카드와 일치하는 구문을 찾는다. 이때 입력된 검색어는 형태소 분석이 이뤄지지 않는다.

**Nested Query**: 분산시틈에서 SQL에 Join 과 유사한 기능을 수행하려면 매우 많은 비용이 소모 될 것 이다. 수평적인 샤드가 얼마나 늘어날지 모르는 상황에서 모든 샤드를 검색해야 할 수도 있기 때문이다. 하지만 업무를 수행하다 보면 문서 간의 부모/자식 관계의 형태로 모델링 되는 경우가 종종 발생할 것이다.

**Search Shards API**: 검색이 수행되는 노드 및 샤드에 대한 정보를 확인할 수 있따. 이러한 정보는 질의를 최적화 하거나 질의가 정상적으로 수행되지 않을 때 문제를 해결하는데 유용하게 활용할 수 있다.

**Multi Search API**: 웹 서버로 요청되는 횟수를 줄이는것이 성능을 향상시크는 중요한 효건이라는 것을 알고 있음. Multi Search API는 여러건의 검색ㅇ 요청을 통합해서 한번에 요청하고 한번에 결과를 종합해서 받을 때 사용되는 API

**Count API**: 간혹 문서 본문보다 검색된 문서의 개수가  몇 개인지 그 숫자만 필요할 떄가 있다. 이럴 때는 엘라스틱 서치에서 제공하는 Count API를 사요ㅗㅇ하면 된다.

**Validate API**: 쿼리 실행기에 앞서 쿼리가 유효하게 작성 됐는지 검증하는 것이 가능하다. URI 검색과 Reqeusts Body 검색에서 모두 사용할 수 있따.

**Explain API**: 문서 검색 결과를 확인해 보면 _score를 통해 우리가 검색한 키워드와 검색 결과가 얼마나 유사한지 확인할 수 있다. 만일 이 문서가 가진 _socre 값이 어떻게 계산된 것인지 자세한 정보를 알고 싶다면 어떻게 해야 할까? Explain API를 사용하면 특정 문서에 대해 요청한 쿼리가 어떻게 스코어가 계산되는지 자세하게 확인 할 수 있다.